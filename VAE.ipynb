{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/wx07bTjmqesdCAKeFBtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhoshsuresh/Chicago-Crime-Analysis---Django-Python/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eajwHNcMSwZt"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import nn\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.datasets import MNIST # Training dataset\r\n",
        "from torchvision.utils import make_grid\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "torch.manual_seed(0) # Set for testing purposes, please do not change!\r\n",
        "\r\n",
        "# Function for visualizing images to plot the images in a uniform grid\r\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\r\n",
        "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\r\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\r\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ocP9PWS0BW"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        c = capacity\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\r\n",
        "        self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\r\n",
        "        self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\r\n",
        "            \r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.conv1(x))\r\n",
        "        x = F.relu(self.conv2(x))\r\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\r\n",
        "        x_mu = self.fc_mu(x)\r\n",
        "        x_logvar = self.fc_logvar(x)\r\n",
        "        return x_mu, x_logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUzapmbOS_Bx"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        c = capacity\r\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\r\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\r\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\r\n",
        "            \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.fc(x)\r\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\r\n",
        "        x = F.relu(self.conv2(x))\r\n",
        "        x = torch.sigmoid(self.conv1(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJzHBsrZTBsF"
      },
      "source": [
        "class VariationalAutoencoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(VariationalAutoencoder, self).__init__()\r\n",
        "        self.encoder = Encoder()\r\n",
        "        self.decoder = Decoder()\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        latent_mu, latent_logvar = self.encoder(x)\r\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\r\n",
        "        x_recon = self.decoder(latent)\r\n",
        "        return x_recon, latent_mu, latent_logvar\r\n",
        "    \r\n",
        "    def latent_sample(self, mu, logvar):\r\n",
        "        if self.training:\r\n",
        "            # the reparameterization trick\r\n",
        "            std = logvar.mul(0.5).exp_()\r\n",
        "            eps = torch.empty_like(std).normal_()\r\n",
        "            return eps.mul(std).add_(mu)\r\n",
        "        else:\r\n",
        "            return mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYLTomc9TO4s"
      },
      "source": [
        "def computeVAELoss(recon_x, x, mu, logvar):\r\n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\r\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\r\n",
        "    \r\n",
        "    return recon_loss + variational_beta * kldivergence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou6fEAA5TEz6"
      },
      "source": [
        "latent_dims = 2\r\n",
        "num_epochs = 50\r\n",
        "batch_size = 128\r\n",
        "capacity = 64\r\n",
        "learning_rate = 1e-3\r\n",
        "variational_beta = 1\r\n",
        "use_gpu = True\r\n",
        "display_step = 500\r\n",
        "\r\n",
        "img_transform = transforms.Compose([\r\n",
        "    transforms.ToTensor()\r\n",
        "])\r\n",
        "\r\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftRiPTk2TJKC",
        "outputId": "6256f1be-d7de-4ff0-df09-5969baebcb51"
      },
      "source": [
        "vae = VariationalAutoencoder()\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\r\n",
        "vae = vae.to(device)\r\n",
        "\r\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\r\n",
        "print('Number of parameters: %d' % num_params)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(params=VAE.parameters(), lr=learning_rate, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 308357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OaXVWJNTSes",
        "outputId": "4f7ed81d-a17d-4093-b1eb-0b8d1f7f8e8f"
      },
      "source": [
        "# set to training mode\r\n",
        "VAE.train()\r\n",
        "\r\n",
        "train_loss_avg = []\r\n",
        "\r\n",
        "print('Training ...')\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    train_loss_avg.append(0)\r\n",
        "    num_batches = 0\r\n",
        "    \r\n",
        "    for image_batch, _ in train_dataloader:\r\n",
        "        \r\n",
        "        image_batch = image_batch.to(device)\r\n",
        "\r\n",
        "        # vae reconstruction\r\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\r\n",
        "        \r\n",
        "        # reconstruction error\r\n",
        "        loss = computeVAELoss(image_batch_recon, image_batch, latent_mu, latent_logvar)\r\n",
        "        \r\n",
        "        # backpropagation\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        # one step of the optmizer (using the gradients from backpropagation)\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        train_loss_avg[-1] += loss.item()\r\n",
        "        num_batches += 1\r\n",
        "        \r\n",
        "    train_loss_avg[-1] /= num_batches\r\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "Epoch [1 / 50] average reconstruction error: 65634.794826\n",
            "Epoch [2 / 50] average reconstruction error: 65639.584055\n",
            "Epoch [3 / 50] average reconstruction error: 65639.290870\n",
            "Epoch [4 / 50] average reconstruction error: 65638.949144\n",
            "Epoch [5 / 50] average reconstruction error: 65636.857409\n",
            "Epoch [6 / 50] average reconstruction error: 65639.546317\n",
            "Epoch [7 / 50] average reconstruction error: 65630.238131\n",
            "Epoch [8 / 50] average reconstruction error: 65636.844266\n",
            "Epoch [9 / 50] average reconstruction error: 65636.125358\n",
            "Epoch [10 / 50] average reconstruction error: 65641.245178\n",
            "Epoch [11 / 50] average reconstruction error: 65630.791220\n",
            "Epoch [12 / 50] average reconstruction error: 65637.908407\n",
            "Epoch [13 / 50] average reconstruction error: 65638.616913\n",
            "Epoch [14 / 50] average reconstruction error: 65640.912488\n",
            "Epoch [15 / 50] average reconstruction error: 65632.080607\n",
            "Epoch [16 / 50] average reconstruction error: 65637.227820\n",
            "Epoch [17 / 50] average reconstruction error: 65633.984009\n",
            "Epoch [18 / 50] average reconstruction error: 65634.809377\n",
            "Epoch [19 / 50] average reconstruction error: 65638.227937\n",
            "Epoch [20 / 50] average reconstruction error: 65636.767815\n",
            "Epoch [21 / 50] average reconstruction error: 65629.387477\n",
            "Epoch [22 / 50] average reconstruction error: 65641.956823\n",
            "Epoch [23 / 50] average reconstruction error: 65646.205640\n",
            "Epoch [24 / 50] average reconstruction error: 65634.919485\n",
            "Epoch [25 / 50] average reconstruction error: 65631.889284\n",
            "Epoch [26 / 50] average reconstruction error: 65635.265150\n",
            "Epoch [27 / 50] average reconstruction error: 65633.902494\n",
            "Epoch [28 / 50] average reconstruction error: 65641.065657\n",
            "Epoch [29 / 50] average reconstruction error: 65634.253490\n",
            "Epoch [30 / 50] average reconstruction error: 65637.304446\n",
            "Epoch [31 / 50] average reconstruction error: 65631.553655\n",
            "Epoch [32 / 50] average reconstruction error: 65633.160165\n",
            "Epoch [33 / 50] average reconstruction error: 65635.224031\n",
            "Epoch [34 / 50] average reconstruction error: 65640.035215\n",
            "Epoch [35 / 50] average reconstruction error: 65635.390217\n",
            "Epoch [36 / 50] average reconstruction error: 65641.795834\n",
            "Epoch [37 / 50] average reconstruction error: 65636.439690\n",
            "Epoch [38 / 50] average reconstruction error: 65632.171192\n",
            "Epoch [39 / 50] average reconstruction error: 65643.762777\n",
            "Epoch [40 / 50] average reconstruction error: 65629.560110\n",
            "Epoch [41 / 50] average reconstruction error: 65637.050981\n",
            "Epoch [42 / 50] average reconstruction error: 65635.483126\n",
            "Epoch [43 / 50] average reconstruction error: 65639.641866\n",
            "Epoch [44 / 50] average reconstruction error: 65639.381422\n",
            "Epoch [45 / 50] average reconstruction error: 65633.134520\n",
            "Epoch [46 / 50] average reconstruction error: 65629.820221\n",
            "Epoch [47 / 50] average reconstruction error: 65631.336454\n",
            "Epoch [48 / 50] average reconstruction error: 65635.655134\n",
            "Epoch [49 / 50] average reconstruction error: 65636.913729\n",
            "Epoch [50 / 50] average reconstruction error: 65642.764709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjowUeHJXUbn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a4b53726-fb6d-4671-cf00-0fa0c49aeb30"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "    z = torch.randn(1, 2).cuda()\r\n",
        "    sample = VAE.decoder(z)\r\n",
        "\r\n",
        "    show_tensor_images(sample, num_images=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwFLAJOZfttC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}